{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f837d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# 1. Load Dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\"\n",
    "df = pd.read_csv(url)\n",
    "X = df.drop(['status', 'name'], axis=1)\n",
    "y = df['status']\n",
    "\n",
    "# 2. Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 4. Define Basic Models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'SVM': SVC(probability=True),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'Bagging': BaggingClassifier()\n",
    "}\n",
    "\n",
    "# 5. Train & Evaluate Basic Models\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    results.append([name, acc, prec, rec, f1])\n",
    "\n",
    "# 6. Hyperparameter Tuning\n",
    "param_grid_lr = {'C': [0.01, 0.1, 1, 10], 'penalty': ['l2']}\n",
    "grid_lr = GridSearchCV(LogisticRegression(), param_grid_lr, cv=5)\n",
    "grid_lr.fit(X_train, y_train)\n",
    "\n",
    "param_grid_rf = {'n_estimators': [50, 100, 150], 'max_depth': [4, 6, 8, None]}\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=5)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "param_grid_svc = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'kernel': ['rbf', 'linear']}\n",
    "grid_svc = GridSearchCV(SVC(probability=True), param_grid_svc, cv=5)\n",
    "grid_svc.fit(X_train, y_train)\n",
    "\n",
    "# 7. Evaluate Tuned Models\n",
    "y_pred_lr = grid_lr.predict(X_test)\n",
    "results.append([\"LightGBM\", accuracy_score(y_test, y_pred_lr), precision_score(y_test, y_pred_lr), recall_score(y_test, y_pred_lr), f1_score(y_test, y_pred_lr)])\n",
    "\n",
    "y_pred_rf = grid_rf.predict(X_test)\n",
    "results.append([\"Random Forest (Tuned)\", accuracy_score(y_test, y_pred_rf), precision_score(y_test, y_pred_rf), recall_score(y_test, y_pred_rf), f1_score(y_test, y_pred_rf)])\n",
    "\n",
    "y_pred_svc = grid_svc.predict(X_test)\n",
    "results.append([\"SVM (Tuned)\", accuracy_score(y_test, y_pred_svc), precision_score(y_test, y_pred_svc), recall_score(y_test, y_pred_svc), f1_score(y_test, y_pred_svc)])\n",
    "\n",
    "# 8. Stacking Classifier\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100)),\n",
    "    ('svc', SVC(probability=True))\n",
    "]\n",
    "stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "stack.fit(X_train, y_train)\n",
    "y_pred_stack = stack.predict(X_test)\n",
    "results.append([\"Stacking Classifier\", accuracy_score(y_test, y_pred_stack), precision_score(y_test, y_pred_stack), recall_score(y_test, y_pred_stack), f1_score(y_test, y_pred_stack)])\n",
    "\n",
    "# 9. Results DataFrame\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"])\n",
    "results_df.to_csv('model_results.csv', index=False)\n",
    "\n",
    "print(\"Training and evaluation completed!\\n\")\n",
    "print(results_df)\n",
    "\n",
    "# 10. Plot Confusion Matrix\n",
    "\n",
    "def plot_confusion(model_name, y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot for basic models\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    plot_confusion(name, y_test, y_pred)\n",
    "\n",
    "# Plot for tuned models\n",
    "plot_confusion(\"Logistic Regression (Tuned)\", y_test, y_pred_lr)\n",
    "plot_confusion(\"Random Forest (Tuned)\", y_test, y_pred_rf)\n",
    "plot_confusion(\"SVM (Tuned)\", y_test, y_pred_svc)\n",
    "plot_confusion(\"Stacking Classifier\", y_test, y_pred_stack)\n",
    "\n",
    "# 11. Plot ROC Curves\n",
    "def plot_roc(model_name, model, X_test, y_test):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_score = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_score = model.decision_function(X_test)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for name, model in models.items():\n",
    "    plot_roc(name, model, X_test, y_test)\n",
    "\n",
    "plot_roc(\"Logistic Regression (Tuned)\", grid_lr.best_estimator_, X_test, y_test)\n",
    "plot_roc(\"Random Forest (Tuned)\", grid_rf.best_estimator_, X_test, y_test)\n",
    "plot_roc(\"SVM (Tuned)\", grid_svc.best_estimator_, X_test, y_test)\n",
    "plot_roc(\"Stacking Classifier\", stack, X_test, y_test)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=\"Accuracy\", y=\"Model\", data=results_df.sort_values(by=\"Accuracy\", ascending=False), palette=\"viridis\")\n",
    "plt.title(\"Model Accuracy Comparison\")\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.xlim(0.7, 1.0)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
